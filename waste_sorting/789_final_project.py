# -*- coding: utf-8 -*-
"""789_Final_Project.ipynb

Automatically generated by Colaboratory.
"""

import numpy as np
import pandas as pd
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')
! cp "/content/drive/MyDrive/dataset.py" .

import dataset

#Prepare input data
train_path="/content/drive/MyDrive/dataset-resized/"
classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']
print(classes)
num_classes = len(classes)

# 10% of the data will automatically be used for validation
validation_size = 0.1
img_size = 48
num_channels = 3
sample_size = 7000

data = dataset.read_train_sets(train_path, img_size, classes, validation_size=validation_size, sample_size=sample_size)


print("Complete reading input data. Will Now print a snippet of it")
print("Number of files in Training-set:\t\t{}".format(len(data.train.labels)))
print("Number of files in Validation-set:\t{}".format(len(data.valid.labels)))

from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization
from keras.optimizers import SGD
from tensorflow import keras
from tensorflow.keras import layers
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(48, 48, 3)))
model.add(BatchNormalization())
model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(6, activation='softmax'))

model.summary()

optimizer = SGD(lr = 0.001, momentum = 0.8)
model.compile(optimizer, loss='categorical_crossentropy', metrics=['accuracy'])

x_train, _, _, _ = data.train.next_batch(2275)
x_valid, _, _, _ = data.valid.next_batch(252)

y_train = data.train.labels
y_valid = data.valid.labels

history = model.fit(x_train, y_train, epochs = 150, batch_size = 64, validation_data=(x_valid, y_valid), verbose = 1)
# Evaluate the model
test_score = model.evaluate(x_valid, y_valid)
print("Test loss {:.4f}, accuracy {:.2f}%".format(test_score[0], test_score[1] * 100))

test_score = model.evaluate(x_valid, y_valid)
print("Test loss {:.4f}, accuracy {:.2f}%".format(test_score[0], test_score[1] * 100))

predictions = model.predict(x_valid)

from sklearn.metrics import confusion_matrix
confusion_matrix(predictions.argmax(axis=1), y_valid.argmax(axis=1))

from matplotlib import pyplot as plt
plt.subplot(211)
plt.title('Cross Entropy Loss')
plt.plot(history.history['loss'], color='blue', label='train')
plt.plot(history.history['val_loss'], color='orange', label='test')
	# plot accuracy
plt.subplot(212)
plt.title('Classification Accuracy')
plt.plot(history.history['accuracy'], color='blue', label='train')
plt.plot(history.history['val_accuracy'], color='orange', label='test')
