# -*- coding: utf-8 -*-
"""Cifar10.ipynb

Automatically generated by Colaboratory.
"""

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np

#test if GPU is on
tf.test.gpu_device_name()

#load dataset
(X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()

#Ck shape of X_train
X_train.shape

#Ck shape of y_train
y_train.shape

#Ck shape of X_test 
X_test.shape

#Ck shape of y_test
y_test.shape

#create new X and y train datasets with only animal items
X_newlist1 = []
y_newlist1 = []
for i in range (50000): 
  for n in range (2,8):
    if y_train [i] == n:
      X_newlist1.append(X_train[i])
      y_newlist1.append(y_train[i])
X_new_train = np.array(X_newlist1)
y_new_train = np.array(y_newlist1)

#ck shape of new X train dataset
X_new_train.shape

#ck shape of new y train dataset 
y_new_train.shape

#create new X and y tset datasets with only animal items
X_newlist2 = []
y_newlist2 = []
for i in range (10000): 
  for n in range (2,8):
    if y_test [i] == n:
      X_newlist2.append(X_test[i])
      y_newlist2.append(y_test[i])
X_new_test = np.array(X_newlist2)
y_new_test = np.array(y_newlist2)

#Ck shape of new X test dataset
X_new_test.shape

#ck shape of new y test dataset 
y_new_test.shape

#plot image
plt.imshow(X_new_train[5000])

#normalization
X_new_train = X_new_train/255
X_new_test = X_new_test/255

#build the CNN model
cnn = models.Sequential([
      #Conv 1                 
      layers.Conv2D(filters=32, kernel_size=(3,3), padding = 'same', activation='relu', input_shape=(32,32,3)),
      #Conv 2
      layers.Conv2D(filters=32, kernel_size=(3,3), padding = 'same', activation='relu'),
      #pooling 1
      layers.MaxPooling2D((2,2)),
      #dropout 1
      layers.Dropout(0.25),

      #batch normolization
      layers.BatchNormalization(),

      #Conv 3                 
      layers.Conv2D(filters=32, kernel_size=(3,3), padding = 'same', activation='relu'),
      #Conv 4
      layers.Conv2D(filters=32, kernel_size=(3,3), padding = 'same', activation='relu'),
      #batch normolization
      layers.BatchNormalization(),
      #pooling 2
      layers.MaxPooling2D((2,2)),
      #dropout 2
      layers.Dropout(0.25),

      layers.Flatten(),

      #batch normolization
      layers.BatchNormalization(),
      
      #Fully Connected Layer
      layers.Dense(32, activation='relu'),
      layers.Dropout(0.25),
      layers.Dense(8, activation='softmax')                   
])

cnn.summary()

#compile model
cnn.compile(optimizer = 'adam',
            loss='sparse_categorical_crossentropy',
            metrics=['sparse_categorical_accuracy'])

#train model
cnn.fit(X_new_train, y_new_train, epochs=20, verbose = 1, validation_data=(X_new_test, y_new_test))

#evaluste model
cnn.evaluate(X_new_test, y_new_test)

y_pred = cnn.predict_classes(X_new_test)

from sklearn.metrics import confusion_matrix , classification_report
print("Classification Report: \n", classification_report(y_new_test, y_pred))
